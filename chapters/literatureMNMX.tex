%!TeX root= ../thesis.tex
\newpage
\section{Preference elicitation}
\label{sec:litMNMX}

All the methods described so far assume complete information about the preference orders of all voters over the entire set of alternatives, and about the voting mechanism itself. But how reasonable is this assumption?
If the set of alternatives is very large, it is unreasonable to expect voters to provide a complete order of preferences. First from a cognitive point of view: experimental psychological studies describe how in many situations we do not construct \commentOC{The word “construct” is not ideal here, although probably arguably technically correct: the authors argue that people construct their preferences, opposing the idea that preferences are there from the start.}\commentBN{I'm not sure if you're suggesting to change the sentence because it' s not clear that's what I meant or because you don't think it's appropriate to include this information.} our preferences until we are in the situation of having to express them \citep{Lichtenstein2006}. Asking voters to express preferences when the set of choices is very wide can lead to various problems, such as confusion and low participation.
Second, but no less important, this represents a huge communication cost. \citet{Conitzer2005} studied the communication complexity of some of the most common voting rules and, for each, provided an upper and lower bound on the number of bits of information that voters are required to communicate before the rule can select a winner. Other metrics exist, such as the number of questions voters must answer before the rule can determine a winner. This approach was used by \citet{Procaccia2008} to determine the minimum number of questions to select the Condorcet winner.
In presidential elections, for example, we tend to prefer accuracy over cost, but this is not true in all applications. When it comes to choosing a restaurant to have dinner at with friends, we are likely to settle for an approximate winner rather than having to spend the entire evening ranking all the restaurants in town.
\paragraph{Possible and necessary winners.}
On these basis, \citet{Konczak05} introduced the concepts of possible and necessary winners. Given a partial profile there are many ways to add preferences in order to build a complete profile. If an alternative $a$ is a winner in some of these completion then $a$ is a possible winner in the partial profile. If $a$ is a winner in \textit{all} completion then $a$ is a necessary winner. This information can be very important during an elicitation process because if we reach a necessary winner then we may decide to stop the elicitation process.
Numerous studies have been published on the computational complexity of finding possible and necessary winners \citep{Konczak05,Pini2007,Walsh2007,Xia2008,Xia2011,Baumeister2012}.
See \citet{Lang2020} for a survey on possible and necessary winner not only in voting but also in other social choice scenarios like fair division, partial tournaments and hedonic games.

Although being an important concept from a theoretical point of view, the problem still persists. It is often necessary to know all, or almost all, preference rankings in order to compute a necessary winner. As for possible winners, yes, they can be used to reduce the set of alternatives, but how to choose the right approximation?
\citet{Boutilier2006}, and later \citet{Lu2011}, proposed the use of minimax regret: the alternative $a$ that minimizes the regret of choosing $a$ instead of the winner in the worst case\textemdash which is the winner in the worst possible completion of the preference profile from $a$'s perspective\textemdash is the best "current" approximation.
Our elicitation procedure presented in \Cref{ch:minimax} is based on this approach, but others have been suggested.
For example, \citet{Bachrach2010} proposed a probabilistic method in which they considered for each possible winner the number of completions for which it is a winner.
\citet{Lu2011Prob} suggested a mix of these two strategies to determine a minimal value $k$ such that by asking the voters their best $k$ alternatives is it possible to determine an approximate winner with a low regret.
\paragraph{Building an elicitation procedure.}
This brings us to another point, established that in many cases it is convenient to ask for only partial information rather than to demand the complete preferences from the beginning, how to conduct this elicitation process? And when to stop it?
\citet{Conitzer2002}, and \citet{Walsh2009} later on, studied the complexity of determining when to stop the elicitation process and concluded that it depends on the choice of elicitation procedure and voting rule. The authors proved that for some rules, like \ac{STV}, the problem can be resolved in a polynomial time if voters are asked to provide their full preference rankings but it becomes NP-hard when considering pairwise comparison queries that ask voters to compare two specific alternatives. However, those two problems are both polynomials for some other rules like Borda and plurality.

Although there are some studies investigating the "one-shot" preference elicitation, as the work of \citet{Lu2011Prob} cited earlier, much attention has focused on incremental preference elicitation.
\citet{Kalech2011} proposed both approaches. In their first algorithm, \textit{Iterative Voting}, at each round all voters are asked, at the same time, to provide their (next) best alternative. Possible and necessary winners are computed and once found one (or all) necessary winner(s) the elicitation stops. Their second algorithm, \textit{Greedy Voting}, instead, ask all voters their best $k$ alternatives and then stops the elicitation procedure. However, this method does not guarantee to find a necessary winner.
\citet{Lu2011} also proposed an iterative procedure that use the minimax regret to find the next question to ask, as already mentioned, but also to decide when to stop the elicitation process. In fact, they define a threshold such that if the regret is lower than this threshold the current approximate winner is returned.
\citet{Benabbou2016} extended this work to multi-attribute domains\textemdash settings in which voters express their preferences over a set of interrelated alternatives, e.g. a multiple referendum. The authors provided an iterative elicitation procedure that stops as soon as a Borda winner can be determined and also returns an aggregated ranking of the best $k$ alternatives.
\citet{Naamani-Dery2015} proposed two incremental procedures that differ in their heuristic for choosing the next question. Their first approach selects the query, among the possible ones, that maximizes the probability of information gain. The second mechanism selects first the candidate most likely to win and then identifies the query that maximizes its possibility of winning.

There are other aspects of preference elicitation that we will not cover, for example the complexity of queries, or situations where the set of alternatives is uncertain. For more on these topics, we recommend reading \citet[Ch. 10]{Comsoc2016}.
\paragraph{Uncertainty of the voting rule.}
In this manuscript, we will instead deal with an aspect that has not been treated much in the literature: the uncertainty of the voting rule. It is often assumed that the uncertainty of the voting rule may be due to an attempt of manipulation by the body responsible for defining the procedure. In the concluding remarks of his work, \citet{Walsh2007}, mentioned as a future direction:

\textit{\say{Other forms of uncertainty have yet to be studied in detail. For instance, there may be uncertainty in the weight vector used by a scoring rule. The Chair may want to manipulate the election by collecting all the votes and then deciding on the weights in such a way that ensures that their preferred candidate wins. As a second example, the Chair may want to hinder strategic voting by not announcing the weights in advance or by choosing weights randomly.}}

A similar perspective has been adopted by subsequent works dealing with the uncertainty of the voting rule. These, just like the second part of Walsh consideration, focused on the impact of this uncertainty on voters' manipulation and strategic behavior \citep{Baumeister2011,Elkind2012,Holliday2019}.

However, we do not believe that this is the only interesting scenario. Similarly to the case of preference profile uncertainty where it is difficult to obtain voter preferences from a cognitive and computational point of view, it may be difficult for a committee of non-experts to define an aggregation procedure. 
Consider the example suggested by Walsh where a committee has to formalize a scoring rule. The members of the committee may have an idea about the importance of weights, but it may be very difficult for them to properly define the scoring vector. 
Consider, for example, the board of a company that must decide how to aggregate experts opinions about the candidate to hire. Maybe they consider that being ranked first is "much more" valuable than being ranked second and so on, but how to define this "much more"? Should the first position be worth twice the second position? On which basis can non-experts choose this parameter?
It is important to mention that the problem of how to choose the weights associated with a scoring rule has been extensively studied and it has been shown how a winner with given weights might not be one when changing the scoring vector \citep{Cook1990,Llamazares2013,Llamazares2016}. 

To the best of our knowledge very few works tackle the problem of eliciting a voting rule. We mention the works of \citet{Llamazares2013,Viappiani2018}, that considers scoring rules with uncertain weights, and that of \citet{Cailloux2014} that proposes a framework for explaining the consequences of choosing a specific set of axioms to non-expert users so that they can take an informed decision on which rule to choose.
We use this methodology of reasoning through examples when building questions to the committee to elicit their preference.

In \Cref{ch:minimax}, we propose an elicitation procedure that considers both preference profile and scoring vector as sources of uncertainty at the same time. This represents a novelty in the literature of voting under incomplete preference. It is important to mention that the elicitation process is conducted by a hypothetical third party external to the voting process. The committee cannot be influenced by voters preferences and vice versa. 
We focus on positional scoring rules with convex weights and develop elicitation strategies based on minimax regret. 

\paragraph{Types of questions and preferences inconsistency.}
We consider two kind of questions: the questions to the agents are pairwise comparisons between two alternatives; the questions to the chair ask to select a winner out of an example profile.
Empirical psychological studies show how alternative phrasing of the same question give rise to different preferences \citep{Tversky1986}. 
For example, in an experiment about this phenomenon conducted by \citet{Irwin1993}, several participants were asked to evaluate problems such as:
\begin{itemize}[nosep]
	\item[1] Improving the air quality in Denver.
	\item[2] Upgrading their TV.
\end{itemize}
When asked to choose an alternative in a pairwise comparisons between those two options, an overwhelming majority choose to improve the air quality. However, when participants were presented these options separately, most of them were willing to pay more for upgrading their TV.

There is no best method of asking questions because all of them, in one way or another, are bound to produce inconsistencies. However, we want to provide some considerations on the two chosen formulations.

The \ac{AHP} is a mathematical model, widely used in decision making theory, to make complex decision. It was proposed by \citet{Saaty1986} and it is based on the law of comparative judgment conceived by \citet{Thurstone1927}. The latter is a model to retrieve information through the use of pairwise comparisons. Thus, \ac{AHP} is a model that, to arrive to a complex decision, uses pairwise comparisons between the various alternatives.
Although it has some criticisms, such as the inconsistency of preferences that we mentioned earlier, it is one of the most used decision-making methods. Dividing a complex problem into a hierarchy of smaller, more addressable sub-problems allows users to analyze different aspects at the time, making the choice less complicated.

Regarding the uses of example profile to elicit preferences about the voting rule, other than the work of \citet{Cailloux2014}, we want to mention a real life experiment performed by \citet{Giritligil2005}. The authors of the latter paper, in an attempt to understand how non-experts perceived the majoritarian approval principle, created a survey in which they presented different profiles to 288 participants who had to choose a winner in each one of them. In each profile they accurately created a Majoritarian Compromise-winner, a Borda-winner and a Condorcet-winner, making sure that the first is always distinct from the other two. They concluded that, although many of the participants consistently chose the Borda-winner, the Majoritarian-winner was also well supported.

\paragraph{Preference elicitation in Decision Aiding.}
The problem of information elicitation is extensively studied in the domain of decision aiding. In this context, there is a \textit{\ac{DM}} who seeks the help of an expert party, an \textit{analyst}, to develop recommendations that the \ac{DM} can later use to make her choices. In this process, there is a stage where the analyst has to formalize the problem in the form of a mathematical model. To do so, it is important to get the preferences of the \ac{DM}, who, as a non-expert, often fails to identify the parameters relevant to the problem. It is the job of the analyst to ask the right questions to obtain the preference information.
Different approaches to elicitation have been proposed based on the choice of those questions, but also the availability (and patience) of the \ac{DM}, or whether the \ac{DM} is a real person or a digital database.
Please see \citet{Bouyssou2006} for a more detailed account on the matter and \citet{Mousseau2005}, in the context of Multiple Criteria Decision Aid, and \citet{Belahcene2018} for an overview of these methods. In particular, the latter, much like \citet{Cailloux2014}, proposes a framework for the explanation of the elicitation process itself. In fact, similarly to how \citet{Cailloux2014} tries to elicit the voting rule through concrete examples, \citet{Belahcene2018} tries to do the same to elicit preference information instead of explaining the result of the recommendation.
As a side note, it is also interesting to mention that it is often assumed that in an elicitation process the analyst tries to elicit information that the \ac{DM} has but which is unknown by the analyst. This is not always the case. 
\citet[Ch. 8]{Bouyssou2000} presented a scenario of information uncertainty with a real case example. A company dealing with the production and distribution of electricity\textemdash which here represents the \ac{DM}\textemdash asked the help of a research group\textemdash which embodies the analyst\textemdash for the realization of a decision-aiding tool that would help them with the company choices. Every three years the company had to reevaluate its plan of building or closing its various (coal, nuclear, and gas) power plants. When formalizing the model, the authors realized that the values to assign to the various criteria were mostly uncertain. For example, the fuel price fluctuates according to the market, the electricity demand may depend on the weather, and a specific pollution legislation may have an impact on the company policy. The authors proposed a model based on successive pairwise comparisons: at each step it compares the consequences of two different decision in the same scenario. The model was adopted by the company, however, the authors showed some drawbacks of the model, in particular how it can lead to inconsistencies, and discussed why they did not adopt a probabilistic approach. Although it is not in the scope of our research, because we assume that the unknown information exists somewhere but is unknown to us, this consideration opens the door to many other potentially interesting scenarios to investigate.

\paragraph{Preference Elicitation in Machine Learning.}
In general, when we think of machine learning problems we tend to divide them into classification problems\textemdash in which the goal is to predict the category into which a particular instance falls\textemdash and regression problems\textemdash in which the purpose is to predict the value of a particular variable associated with an instance. 
However, many real-life problems are based on preferences of individuals.
Consider, as an example, the enormous amount of information on the Web. It is impossible for users to find something without efficient Information Retrieval (IR) mechanisms. Given a query submitted by the user, an IR mechanism provides a list of documents ranked in order of relevance. The ranking is the key element that defines a good system. 
We might approach the problem by predicting whether or not a document is relevant to the query, as in a classification problem, or by calculating the probability that a document is relevant, like a regression problem. However, we are leaving out an important piece of information that is the correlation between the documents that are not independent variables but are all potential answers to the query. This makes ranking a fundamentally different problem than classification and regression.
Document retrieval is only one example of the use of rankings in IR, a detailed account of which can be found in \citet{Liu2009}, but other uses can be found like recommender systems.
%Some other applications are: collaborative filtering\textemdash a technique for the prediction of an individual's preferences from the preferences of other individuals\textemdash, sentiment analysis and product rating. %i don't know how any references for those tho

The uses of preferences and rankings play a key role in many machine learning problems. The process of learning preference models from already existing and, available, preference information is denoted as \textit{Preference Learning}. \citet{Furnkranz2010} provided a comprehensive overview of this sub-field of machine learning.
Given a set of items and a set of preferences over these items, the goal of a \textit{preference learning task} is to learn a function that predicts preferences for a different set of items producing, as a result, a total order of the whole set of items\textemdash this task is denoted as \textit{Object Ranking}\textemdash, or that predicts preferences of new users over the same set of items\textemdash denoted as \textit{Label Ranking}.
The training preference information is usually given as a set of pairwise preferences.
This looks quite similar to our elicitation problem, where we have incomplete information that we need to learn. However, there are many differences between the two approaches.
First, in elicitation we try to learn about each individual preferences in order to find the most accurate approximation, in preference learning, on the other hand, the goal is to find a good prediction that is good on average on the entire population. Moreover, preference elicitation often assumes that, as seen in the psychological studies aforementioned, preferences are constructed concurrently with the elicitation process. This involves a third party, a generic external party as in our approach in \Cref{ch:minimax} that could represent the analyst in decision aiding, who communicates interactively with each individual. In cases of inconsistencies this party can notice and correct them, perhaps by asking clarifying questions to the individuals who produced such inconsistency.
In preference learning, on the contrary, a ground truth is assumed and the quality of the prediction is measured based on its differences from the truth. Moreover, these preferences exist in a database and are always accessible but there is no interactive process that can help correct inconsistencies.

There are, however, works that position themselves between these two approaches filling the gaps. For example, in \Cref{ch:minimax} we assume the existence of a ground truth, and \citet{Bachrach2010,Lu2011Prob} studied preference elicitation in a probabilistic setting. On the machine learning side, many studies try to put "humans-in-the-loop" to receive feedback on predictions and, in general, to get more accurate results \citep{Wu2022}.




